
!pip install tensorflow matplotlib

from tensorflow import keras
from tensorflow.keras import layers

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Check size of each set
print("Train X:", x_train.shape)
print("Train Y:", y_train.shape)
print("Test X:", x_test.shape)
print("Test Y:", y_test.shape)

# Verify random sample
import matplotlib.pyplot as plt
sampleNo = 1002
plt.imshow(x_train[sampleNo], cmap='gray', )
print("Ground Truth Value:", y_train[sampleNo])

x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

x_train = x_train.reshape((-1, 28 * 28))
x_test = x_test.reshape((-1, 28 * 28))
# Verify shape
print(x_train.shape) # should be 60000x784
print(x_test.shape) # should be 10000x784

model = keras.Sequential([
keras.layers.Input((28 * 28,)),
keras.layers.Dense(512, activation="relu"),
keras.layers.Dense(256, activation="relu"),
keras.layers.Dense(128, activation="relu"),
keras.layers.Dense(10, activation="softmax")
])

model.compile(
optimizer="adam",
loss="sparse_categorical_crossentropy",
metrics=["accuracy"]
)

history = model.fit(x_train, y_train, batch_size=32, validation_split=0.1, epochs=2)

import pandas as pd
history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot()
history_df.loc[:, ['accuracy', 'val_accuracy']].plot()
plt.show()

import numpy as np
y_pred_prob = model.predict(x_test[:5])
y_pred_classes = np.argmax(y_pred_prob, axis=1)
print("Predicted", y_pred_classes)
print("Ground Truth Value::", y_test[:5])

test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {test_acc:.4f}")

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Get predictions for the entire test set
y_predicted = model.predict(x_test)  # Should be shape (num_samples, num_classes)
y_predicted_classes = np.argmax(y_predicted, axis=1)  # Shape (num_samples,)

# Check shapes
print("y_test shape:", y_test.shape)
print("y_predicted_classes shape:", y_predicted_classes.shape)

# Generate confusion matrix
cm = confusion_matrix(y_test, y_predicted_classes)

# Plot confusion matrix
plt.figure(figsize=(10, 8))
seaborn.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

from sklearn.metrics import precision_recall_curve, auc
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize
# Binarize the output labels for multiclass
y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
# Get prediction probabilities for the entire test set
y_predicted = model.predict(x_test)
# For each class
precision = dict()
recall = dict()
average_precision = dict()
for i in range(10):
    precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i], y_predicted[:, i])
    average_precision[i] = auc(recall[i], precision[i])
# Plot Precision-Recall curve for each class
plt.figure(figsize=(10, 8))
for i in range(10):
    plt.plot(recall[i], precision[i], lw=2, label='Class {0} (AP = {1:0.2f})'.format(i, average_precision[i]))
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve per Class")
plt.legend(loc="lower left")
plt.show()

from sklearn.metrics import f1_score
f1score = f1_score(y_test, y_predicted_classes, average='weighted')
print(f"F1 Score: {f1score:.4f}")
