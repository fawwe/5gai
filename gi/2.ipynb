{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4815d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "imprt gensim.downloader as api\n",
    "imprt matplotlib.pyplot as plt\n",
    "from sklearn.decomposition imprt PCA\n",
    "imprt numpy as np\n",
    "\n",
    "model= api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "def visualize_embeddings(words)))\n",
    "    valid_words =[word for word in words if word in model.key_to_index]\n",
    "\n",
    "    if not valid_words:\n",
    "        print(\"No valid words in the model.\")\n",
    "        return\n",
    "\n",
    "    vectors =np.array([model[word] for word in valid_words])\n",
    "    pca =PCA(n_components=2)\n",
    "    reduced_vectors =pca.fit_transform(vectors)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for i, word in enumerate(valid_words)))\n",
    "        plt.scatter(reduced_vectors[i, 0],reduced_vectors[i, 1], label=word)\n",
    "        plt.text(reduced_vectors[i, 0] + 0.02, reduced_vectors[i, 1] + 0.02,word,fontsize=12)\n",
    "\n",
    "    plt.title(\"Word Embedding Visualization\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "words_to_visualize = [\"king\",\"queen\",\"man\",\"woman\", \"teacher\",\"student\", \"ocean\",\"rain\", \"water\",\"knowledge\"]\n",
    "\n",
    "visualize_embeddings(words_to_visualize)\n",
    "\n",
    "\n",
    "imprt gensim.downloader as api\n",
    "imprt matplotlib.pyplot as plt\n",
    "from sklearn.decomposition imprt PCA\n",
    "imprt numpy as np\n",
    "\n",
    "model =api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "def visualize_embeddings()))\n",
    "    words = [\"AI\", \"data\", \"algorithm\",\"technology\",\"science\",\"automation\",\"machine\"]\n",
    "    valid_words = [word for word in words if word in model]\n",
    "\t\n",
    "    vectors =np.array([model[word] for word in valid_words])\n",
    "    pca =PCA(n_components=2)\n",
    "    reduced_vectors= pca.fit_transform(vectors)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "\tfor i, word in enumerate(valid_words)))\n",
    "        plt.scatter(reduced_vectors[i, 0],reduced_vectors[i, 1])\n",
    "        plt.text(reduced_vectors[i, 0] + 0.02, reduced_vectors[i, 1] + 0.02, word,fontsize=12)\n",
    "\n",
    "    plt.title(\"Word Embedding Visualization using PCA\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.show()\n",
    "\n",
    "def find_similar_words(word)))\n",
    "    similar_words =model.most_similar(word, topn=5)\n",
    "    print(f\"Words similar to'{word}':\")\n",
    "    for similar_word, score in similar_words:\n",
    "        print(f\"{similar_word}(score: {score:.4f})\")\n",
    "\n",
    "word = input()\n",
    "visualize_embeddings()\n",
    "find_similar_words(word)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
